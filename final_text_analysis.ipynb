{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scientist Evaluation\n",
    "# Chelsea Zerrenner\n",
    "\n",
    "The following script seeks to classify Stack Exchange Posts into one of 5 categories: \"astronomy,\" \"aviation,\" \"beer,\" \"outdoors,\" and \"pets\".  The script was written according to the steps laid out by the evaluation instructions.  Note: The first attempt was completed following the instructions explicitly.  It came to my attention, however, that certain steps were not taken or were done out of the sequence when it comes to data science \"best practices.\"  As such, I have provided additional attempts taking this into account.  \n",
    "***\n",
    "## Attempt 1:\n",
    "This attempt follows the instructions explicitly in the order they were laid out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset from Stack Exchange Posts\n",
    "Create connection to MongoDB to access the **stack** database and extract the following collections: **astronomy**, **aviation**, **beer**, **outdoors**, and **pets**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "client = MongoClient()\n",
    "db = client.stack\n",
    "\n",
    "astronomy = pd.DataFrame(list(db.astronomy.find()))\n",
    "aviation = pd.DataFrame(list(db.aviation.find()))\n",
    "beer = pd.DataFrame(list(db.beer.find()))\n",
    "outdoors = pd.DataFrame(list(db.outdoors.find()))\n",
    "pets = pd.DataFrame(list(db.pets.find()))\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine collections to build one complete dataset named **posts** and create label based on the topic of the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.concat([astronomy, aviation, beer, outdoors, pets])\n",
    "\n",
    "def f(x):\n",
    "    if x['id'].startswith('astronomy'): return 'astronomy'\n",
    "    elif x['id'].startswith('aviation'): return 'aviation'\n",
    "    elif x['id'].startswith('beer'): return 'beer'\n",
    "    elif x['id'].startswith('outdoors'): return 'outdoors'\n",
    "    else: return 'pets'\n",
    "    \n",
    "posts['label'] = posts.apply(f, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the dataset to the **title**, **body**, and **label** inputs.  Separate the label data (outputs) from the text (inputs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = posts['title'] + posts['body']\n",
    "y = posts['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    How do I calculate the inclination of an objec...\n",
       "1    How are the compositional components of exopla...\n",
       "2    Amateur observing targets for binary star syst...\n",
       "3    Why do sunspots appear dark? Sunspots, such as...\n",
       "4    Why can't light escape from a black hole? I've...\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Firs, remove punctuation from the text in the **X** dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chzerr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    How do I calculate the inclination of an objec...\n",
       "1    How are the compositional components of exopla...\n",
       "2    Amateur observing targets for binary star syst...\n",
       "3    Why do sunspots appear dark Sunspots such as t...\n",
       "4    Why cant light escape from a black hole Ive he...\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "import string\n",
    "\n",
    "def remove_punc(text):\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    return nopunc\n",
    "       \n",
    "X = X.apply(remove_punc)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the 1000 most common words in the **X** dataset.  Follow up with creating a term document matrix **tdm**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chzerr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chzerr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('wa', 2009),\n",
       " ('dog', 1784),\n",
       " ('cat', 1760),\n",
       " ('like', 1745),\n",
       " ('im', 1702),\n",
       " ('doe', 1667),\n",
       " ('time', 1566),\n",
       " ('ha', 1448),\n",
       " ('know', 1383),\n",
       " ('just', 1350),\n",
       " ('aircraft', 1099),\n",
       " ('ive', 1077),\n",
       " ('question', 1021),\n",
       " ('use', 974),\n",
       " ('way', 970),\n",
       " ('dont', 963),\n",
       " ('star', 919),\n",
       " ('make', 877),\n",
       " ('year', 849),\n",
       " ('flight', 834),\n",
       " ('beer', 828),\n",
       " ('day', 790),\n",
       " ('need', 788),\n",
       " ('water', 737),\n",
       " ('good', 709),\n",
       " ('used', 704),\n",
       " ('pilot', 703),\n",
       " ('want', 699),\n",
       " ('food', 676),\n",
       " ('thing', 631),\n",
       " ('really', 590),\n",
       " ('plane', 579),\n",
       " ('earth', 576),\n",
       " ('light', 574),\n",
       " ('different', 559),\n",
       " ('possible', 558),\n",
       " ('u', 540),\n",
       " ('long', 524),\n",
       " ('say', 498),\n",
       " ('planet', 482),\n",
       " ('looking', 478),\n",
       " ('people', 477),\n",
       " ('new', 470),\n",
       " ('area', 464),\n",
       " ('old', 463),\n",
       " ('away', 461),\n",
       " ('point', 461),\n",
       " ('using', 460),\n",
       " ('think', 454),\n",
       " ('small', 451),\n",
       " ('work', 449),\n",
       " ('doesnt', 439),\n",
       " ('black', 438),\n",
       " ('rabbit', 438),\n",
       " ('look', 436),\n",
       " ('come', 428),\n",
       " ('problem', 424),\n",
       " ('moon', 421),\n",
       " ('place', 414),\n",
       " ('mean', 410),\n",
       " ('sure', 407),\n",
       " ('sun', 404),\n",
       " ('lot', 404),\n",
       " ('month', 404),\n",
       " ('universe', 401),\n",
       " ('fish', 400),\n",
       " ('type', 398),\n",
       " ('help', 397),\n",
       " ('going', 397),\n",
       " ('tent', 397),\n",
       " ('example', 394),\n",
       " ('far', 394),\n",
       " ('best', 389),\n",
       " ('speed', 388),\n",
       " ('climbing', 388),\n",
       " ('2', 387),\n",
       " ('right', 383),\n",
       " ('galaxy', 382),\n",
       " ('difference', 381),\n",
       " ('hole', 370),\n",
       " ('answer', 365),\n",
       " ('pet', 363),\n",
       " ('engine', 360),\n",
       " ('fly', 355),\n",
       " ('did', 348),\n",
       " ('able', 344),\n",
       " ('air', 343),\n",
       " ('case', 340),\n",
       " ('getting', 337),\n",
       " ('start', 337),\n",
       " ('space', 335),\n",
       " ('better', 334),\n",
       " ('got', 329),\n",
       " ('night', 327),\n",
       " ('reason', 326),\n",
       " ('object', 325),\n",
       " ('altitude', 324),\n",
       " ('large', 321),\n",
       " ('outside', 321),\n",
       " ('heard', 320),\n",
       " ('try', 320),\n",
       " ('week', 320),\n",
       " ('house', 317),\n",
       " ('seen', 315),\n",
       " ('safe', 314),\n",
       " ('actually', 313),\n",
       " ('hour', 313),\n",
       " ('stop', 312),\n",
       " ('solar', 310),\n",
       " ('let', 309),\n",
       " ('home', 308),\n",
       " ('understand', 307),\n",
       " ('landing', 307),\n",
       " ('quite', 305),\n",
       " ('le', 305),\n",
       " ('bit', 305),\n",
       " ('bag', 305),\n",
       " ('change', 304),\n",
       " ('hiking', 304),\n",
       " ('high', 299),\n",
       " ('doing', 298),\n",
       " ('3', 298),\n",
       " ('kind', 296),\n",
       " ('number', 296),\n",
       " ('id', 295),\n",
       " ('information', 292),\n",
       " ('little', 292),\n",
       " ('wondering', 291),\n",
       " ('read', 287),\n",
       " ('recently', 286),\n",
       " ('cause', 283),\n",
       " ('rope', 282),\n",
       " ('idea', 280),\n",
       " ('approach', 280),\n",
       " ('training', 275),\n",
       " ('distance', 274),\n",
       " ('usually', 274),\n",
       " ('data', 274),\n",
       " ('having', 273),\n",
       " ('tried', 272),\n",
       " ('flying', 272),\n",
       " ('trip', 271),\n",
       " ('given', 270),\n",
       " ('big', 269),\n",
       " ('airport', 269),\n",
       " ('1', 266),\n",
       " ('temperature', 264),\n",
       " ('airplane', 264),\n",
       " ('inside', 261),\n",
       " ('matter', 257),\n",
       " ('size', 255),\n",
       " ('body', 255),\n",
       " ('effect', 252),\n",
       " ('similar', 251),\n",
       " ('specific', 248),\n",
       " ('control', 248),\n",
       " ('orbit', 247),\n",
       " ('life', 246),\n",
       " ('sleeping', 246),\n",
       " ('tank', 245),\n",
       " ('eat', 242),\n",
       " ('mass', 241),\n",
       " ('trying', 239),\n",
       " ('runway', 239),\n",
       " ('required', 238),\n",
       " ('fuel', 237),\n",
       " ('ground', 237),\n",
       " ('foot', 237),\n",
       " ('experience', 236),\n",
       " ('tell', 235),\n",
       " ('camping', 234),\n",
       " ('general', 232),\n",
       " ('ago', 230),\n",
       " ('normal', 230),\n",
       " ('pretty', 229),\n",
       " ('weight', 228),\n",
       " ('wing', 227),\n",
       " ('land', 226),\n",
       " ('condition', 222),\n",
       " ('maybe', 219),\n",
       " ('box', 218),\n",
       " ('isnt', 217),\n",
       " ('kitten', 217),\n",
       " ('source', 216),\n",
       " ('term', 214),\n",
       " ('state', 214),\n",
       " ('end', 213),\n",
       " ('close', 211),\n",
       " ('hike', 211),\n",
       " ('live', 210),\n",
       " ('issue', 210),\n",
       " ('set', 209),\n",
       " ('whats', 208),\n",
       " ('behavior', 208),\n",
       " ('friend', 208),\n",
       " ('telescope', 207),\n",
       " ('line', 206),\n",
       " ('weather', 205),\n",
       " ('commercial', 205),\n",
       " ('noticed', 202),\n",
       " ('probably', 202),\n",
       " ('5', 202),\n",
       " ('walk', 202),\n",
       " ('bad', 201),\n",
       " ('near', 201),\n",
       " ('second', 201),\n",
       " ('position', 200),\n",
       " ('prevent', 200),\n",
       " ('4', 199),\n",
       " ('rock', 199),\n",
       " ('boot', 198),\n",
       " ('shes', 197),\n",
       " ('feel', 196),\n",
       " ('10', 196),\n",
       " ('level', 195),\n",
       " ('turn', 195),\n",
       " ('hand', 195),\n",
       " ('available', 194),\n",
       " ('mountain', 194),\n",
       " ('surface', 193),\n",
       " ('thought', 192),\n",
       " ('hard', 192),\n",
       " ('gear', 192),\n",
       " ('play', 191),\n",
       " ('situation', 191),\n",
       " ('passenger', 191),\n",
       " ('trail', 190),\n",
       " ('climb', 190),\n",
       " ('run', 189),\n",
       " ('vet', 189),\n",
       " ('cold', 188),\n",
       " ('thats', 188),\n",
       " ('started', 188),\n",
       " ('assume', 186),\n",
       " ('course', 186),\n",
       " ('dry', 184),\n",
       " ('order', 183),\n",
       " ('pack', 182),\n",
       " ('said', 180),\n",
       " ('minute', 180),\n",
       " ('making', 180),\n",
       " ('leave', 179),\n",
       " ('litter', 179),\n",
       " ('gravity', 178),\n",
       " ('didnt', 178),\n",
       " ('plan', 176),\n",
       " ('couple', 176),\n",
       " ('country', 176),\n",
       " ('method', 175),\n",
       " ('true', 175),\n",
       " ('left', 175),\n",
       " ('bottle', 175),\n",
       " ('puppy', 175),\n",
       " ('based', 174),\n",
       " ('moving', 174),\n",
       " ('avoid', 174),\n",
       " ('6', 173),\n",
       " ('short', 173),\n",
       " ('following', 172),\n",
       " ('thinking', 169),\n",
       " ('open', 168),\n",
       " ('walking', 168),\n",
       " ('faa', 167),\n",
       " ('interested', 166),\n",
       " ('atc', 166),\n",
       " ('dark', 165),\n",
       " ('hold', 165),\n",
       " ('currently', 165),\n",
       " ('airline', 165),\n",
       " ('shoe', 165),\n",
       " ('sky', 164),\n",
       " ('equipment', 164),\n",
       " ('common', 164),\n",
       " ('option', 164),\n",
       " ('2014', 163),\n",
       " ('current', 162),\n",
       " ('image', 162),\n",
       " ('jet', 162),\n",
       " ('eye', 161),\n",
       " ('happen', 161),\n",
       " ('direction', 161),\n",
       " ('2013', 161),\n",
       " ('animal', 161),\n",
       " ('longer', 160),\n",
       " ('certain', 160),\n",
       " ('happens', 160),\n",
       " ('head', 160),\n",
       " ('winter', 160),\n",
       " ('energy', 159),\n",
       " ('local', 159),\n",
       " ('human', 159),\n",
       " ('low', 158),\n",
       " ('carry', 158),\n",
       " ('degree', 157),\n",
       " ('bear', 157),\n",
       " ('taking', 156),\n",
       " ('exactly', 156),\n",
       " ('fall', 156),\n",
       " ('feed', 156),\n",
       " ('2012', 156),\n",
       " ('sort', 155),\n",
       " ('pressure', 155),\n",
       " ('great', 154),\n",
       " ('clear', 154),\n",
       " ('especially', 154),\n",
       " ('process', 153),\n",
       " ('instead', 153),\n",
       " ('room', 153),\n",
       " ('v', 153),\n",
       " ('emergency', 153),\n",
       " ('assuming', 152),\n",
       " ('wont', 152),\n",
       " ('came', 152),\n",
       " ('map', 152),\n",
       " ('2010', 151),\n",
       " ('wrong', 151),\n",
       " ('single', 151),\n",
       " ('requirement', 151),\n",
       " ('angle', 150),\n",
       " ('likely', 150),\n",
       " ('theory', 149),\n",
       " ('female', 149),\n",
       " ('stay', 148),\n",
       " ('rule', 148),\n",
       " ('route', 148),\n",
       " ('eating', 148),\n",
       " ('male', 148),\n",
       " ('age', 147),\n",
       " ('treat', 147),\n",
       " ('location', 146),\n",
       " ('2011', 146),\n",
       " ('result', 145),\n",
       " ('spot', 145),\n",
       " ('related', 145),\n",
       " ('door', 145),\n",
       " ('knot', 145),\n",
       " ('affect', 144),\n",
       " ('white', 144),\n",
       " ('buy', 144),\n",
       " ('clean', 143),\n",
       " ('sign', 142),\n",
       " ('radio', 142),\n",
       " ('person', 142),\n",
       " ('car', 142),\n",
       " ('love', 141),\n",
       " ('travel', 140),\n",
       " ('force', 140),\n",
       " ('period', 140),\n",
       " ('important', 140),\n",
       " ('2009', 140),\n",
       " ('class', 139),\n",
       " ('2008', 139),\n",
       " ('edit', 138),\n",
       " ('site', 137),\n",
       " ('running', 137),\n",
       " ('backpacking', 137),\n",
       " ('according', 136),\n",
       " ('particular', 136),\n",
       " ('told', 136),\n",
       " ('care', 136),\n",
       " ('risk', 136),\n",
       " ('considering', 135),\n",
       " ('wind', 135),\n",
       " ('store', 135),\n",
       " ('15', 134),\n",
       " ('wouldnt', 134),\n",
       " ('pole', 134),\n",
       " ('pair', 134),\n",
       " ('generally', 133),\n",
       " ('takeoff', 133),\n",
       " ('obviously', 132),\n",
       " ('picture', 132),\n",
       " ('correct', 132),\n",
       " ('planning', 132),\n",
       " ('sleep', 132),\n",
       " ('known', 131),\n",
       " ('center', 131),\n",
       " ('simply', 131),\n",
       " ('aviation', 131),\n",
       " ('note', 130),\n",
       " ('ask', 130),\n",
       " ('material', 130),\n",
       " ('lead', 130),\n",
       " ('tail', 130),\n",
       " ('train', 130),\n",
       " ('determine', 129),\n",
       " ('standard', 129),\n",
       " ('8', 129),\n",
       " ('range', 129),\n",
       " ('gas', 128),\n",
       " ('specifically', 128),\n",
       " ('rate', 128),\n",
       " ('airliner', 128),\n",
       " ('theyre', 127),\n",
       " ('higher', 127),\n",
       " ('drink', 127),\n",
       " ('toy', 127),\n",
       " ('fact', 126),\n",
       " ('hot', 126),\n",
       " ('consider', 126),\n",
       " ('special', 126),\n",
       " ('real', 126),\n",
       " ('bring', 126),\n",
       " ('private', 126),\n",
       " ('safety', 126),\n",
       " ('reading', 125),\n",
       " ('free', 125),\n",
       " ('list', 125),\n",
       " ('tree', 125),\n",
       " ('considered', 124),\n",
       " ('book', 124),\n",
       " ('august', 124),\n",
       " ('length', 124),\n",
       " ('sound', 124),\n",
       " ('summer', 124),\n",
       " ('north', 123),\n",
       " ('model', 123),\n",
       " ('mar', 123),\n",
       " ('add', 123),\n",
       " ('allowed', 123),\n",
       " ('main', 122),\n",
       " ('smaller', 122),\n",
       " ('group', 122),\n",
       " ('health', 122),\n",
       " ('bite', 122),\n",
       " ('aquarium', 122),\n",
       " ('design', 121),\n",
       " ('called', 121),\n",
       " ('factor', 119),\n",
       " ('actual', 119),\n",
       " ('frequency', 119),\n",
       " ('larger', 118),\n",
       " ('reach', 118),\n",
       " ('june', 118),\n",
       " ('satellite', 117),\n",
       " ('went', 117),\n",
       " ('leg', 117),\n",
       " ('form', 116),\n",
       " ('provide', 116),\n",
       " ('test', 116),\n",
       " ('check', 116),\n",
       " ('explain', 115),\n",
       " ('online', 115),\n",
       " ('various', 115),\n",
       " ('procedure', 115),\n",
       " ('cross', 115),\n",
       " ('asking', 115),\n",
       " ('power', 115),\n",
       " ('calculate', 114),\n",
       " ('difficult', 114),\n",
       " ('reference', 114),\n",
       " ('minimum', 114),\n",
       " ('cost', 114),\n",
       " ('july', 113),\n",
       " ('red', 112),\n",
       " ('youre', 112),\n",
       " ('technique', 112),\n",
       " ('past', 112),\n",
       " ('bed', 112),\n",
       " ('easy', 111),\n",
       " ('wonder', 111),\n",
       " ('wet', 111),\n",
       " ('field', 110),\n",
       " ('30', 110),\n",
       " ('measure', 110),\n",
       " ('view', 110),\n",
       " ('today', 110),\n",
       " ('expect', 110),\n",
       " ('weve', 110),\n",
       " ('havent', 110),\n",
       " ('november', 110),\n",
       " ('product', 110),\n",
       " ('diet', 110),\n",
       " ('atmosphere', 109),\n",
       " ('path', 109),\n",
       " ('fine', 109),\n",
       " ('purpose', 109),\n",
       " ('half', 109),\n",
       " ('stuff', 109),\n",
       " ('20', 109),\n",
       " ('december', 109),\n",
       " ('september', 109),\n",
       " ('wild', 109),\n",
       " ('april', 109),\n",
       " ('park', 109),\n",
       " ('advice', 109),\n",
       " ('fit', 108),\n",
       " ('solution', 108),\n",
       " ('october', 108),\n",
       " ('snow', 108),\n",
       " ('visible', 107),\n",
       " ('device', 107),\n",
       " ('feeding', 107),\n",
       " ('ill', 106),\n",
       " ('morning', 106),\n",
       " ('filter', 106),\n",
       " ('exist', 105),\n",
       " ('mile', 105),\n",
       " ('believe', 105),\n",
       " ('camp', 105),\n",
       " ('regular', 104),\n",
       " ('event', 104),\n",
       " ('thanks', 104),\n",
       " ('guess', 104),\n",
       " ('curious', 104),\n",
       " ('quickly', 104),\n",
       " ('fast', 104),\n",
       " ('glass', 104),\n",
       " ('radar', 104),\n",
       " ('taste', 104),\n",
       " ('aware', 103),\n",
       " ('instrument', 103),\n",
       " ('later', 103),\n",
       " ('dangerous', 103),\n",
       " ('boeing', 103),\n",
       " ('backpack', 103),\n",
       " ('breed', 103),\n",
       " ('asteroid', 102),\n",
       " ('allow', 102),\n",
       " ('article', 102),\n",
       " ('ice', 102),\n",
       " ('effective', 102),\n",
       " ('plastic', 102),\n",
       " ('drinking', 102),\n",
       " ('owner', 102),\n",
       " ('hear', 101),\n",
       " ('mind', 101),\n",
       " ('12', 101),\n",
       " ('starting', 101),\n",
       " ('jump', 101),\n",
       " ('skin', 101),\n",
       " ('turtle', 101),\n",
       " ('activity', 100),\n",
       " ('signal', 100),\n",
       " ('apply', 100),\n",
       " ('heavy', 100),\n",
       " ('coming', 100),\n",
       " ('gps', 100),\n",
       " ('report', 100),\n",
       " ('easily', 99),\n",
       " ('impact', 99),\n",
       " ('concern', 99),\n",
       " ('needed', 99),\n",
       " ('law', 99),\n",
       " ('plant', 99),\n",
       " ('airspace', 99),\n",
       " ('cockpit', 99),\n",
       " ('major', 98),\n",
       " ('quality', 98),\n",
       " ('b', 98),\n",
       " ('operation', 98),\n",
       " ('simple', 97),\n",
       " ('comet', 97),\n",
       " ('figure', 97),\n",
       " ('snake', 97),\n",
       " ('9', 96),\n",
       " ('maximum', 96),\n",
       " ('track', 96),\n",
       " ('taken', 96),\n",
       " ('nice', 96),\n",
       " ('concerned', 96),\n",
       " ('adult', 96),\n",
       " ('video', 95),\n",
       " ('took', 95),\n",
       " ('warm', 95),\n",
       " ('older', 95),\n",
       " ('regulation', 95),\n",
       " ('wear', 95),\n",
       " ('saw', 94),\n",
       " ('basic', 94),\n",
       " ('100', 94),\n",
       " ('increase', 94),\n",
       " ('playing', 94),\n",
       " ('bowl', 94),\n",
       " ('legal', 94),\n",
       " ('cabin', 94),\n",
       " ('rain', 94),\n",
       " ('pond', 94),\n",
       " ('lower', 93),\n",
       " ('imagine', 93),\n",
       " ('follow', 93),\n",
       " ('chance', 93),\n",
       " ('floor', 93),\n",
       " ('talking', 93),\n",
       " ('soon', 93),\n",
       " ('shelter', 93),\n",
       " ('suppose', 92),\n",
       " ('limit', 92),\n",
       " ('world', 92),\n",
       " ('horizon', 92),\n",
       " ('looked', 92),\n",
       " ('heat', 92),\n",
       " ('practice', 92),\n",
       " ('completely', 92),\n",
       " ('working', 92),\n",
       " ('traffic', 92),\n",
       " ('wood', 92),\n",
       " ('finding', 91),\n",
       " ('average', 91),\n",
       " ('sense', 91),\n",
       " ('service', 91),\n",
       " ('happened', 90),\n",
       " ('healthy', 90),\n",
       " ('jupiter', 89),\n",
       " ('safely', 89),\n",
       " ('multiple', 89),\n",
       " ('asked', 89),\n",
       " ('bought', 89),\n",
       " ('radiation', 88),\n",
       " ('natural', 88),\n",
       " ('region', 88),\n",
       " ('putting', 88),\n",
       " ('trekking', 88),\n",
       " ('trek', 88),\n",
       " ('cloud', 87),\n",
       " ('pull', 87),\n",
       " ('relative', 87),\n",
       " ('possibly', 87),\n",
       " ('break', 87),\n",
       " ('shell', 87),\n",
       " ('significant', 87),\n",
       " ('buying', 87),\n",
       " ('astronomy', 86),\n",
       " ('build', 86),\n",
       " ('benefit', 86),\n",
       " ('keeping', 86),\n",
       " ('wife', 86),\n",
       " ('interesting', 85),\n",
       " ('word', 85),\n",
       " ('egg', 85),\n",
       " ('ok', 85),\n",
       " ('recommended', 85),\n",
       " ('license', 85),\n",
       " ('style', 85),\n",
       " ('comfortable', 85),\n",
       " ('pain', 85),\n",
       " ('knife', 85),\n",
       " ('gravitational', 84),\n",
       " ('accident', 84),\n",
       " ('appears', 84),\n",
       " ('value', 84),\n",
       " ('leaving', 84),\n",
       " ('crew', 84),\n",
       " ('contact', 84),\n",
       " ('bird', 84),\n",
       " ('research', 83),\n",
       " ('pick', 83),\n",
       " ('necessary', 83),\n",
       " ('tower', 83),\n",
       " ('company', 83),\n",
       " ('appear', 82),\n",
       " ('lost', 82),\n",
       " ('loop', 82),\n",
       " ('rest', 82),\n",
       " ('slightly', 82),\n",
       " ('noise', 82),\n",
       " ('explanation', 82),\n",
       " ('velocity', 82),\n",
       " ('attack', 82),\n",
       " ('leash', 82),\n",
       " ('shape', 81),\n",
       " ('milky', 81),\n",
       " ('learn', 81),\n",
       " ('andor', 81),\n",
       " ('paper', 81),\n",
       " ('living', 81),\n",
       " ('immediately', 81),\n",
       " ('certificate', 81),\n",
       " ('outdoors', 81),\n",
       " ('cage', 81),\n",
       " ('observed', 80),\n",
       " ('early', 80),\n",
       " ('instance', 80),\n",
       " ('europe', 80),\n",
       " ('uk', 80),\n",
       " ('lift', 80),\n",
       " ('advantage', 80),\n",
       " ('flap', 80),\n",
       " ('eventually', 79),\n",
       " ('x', 79),\n",
       " ('relatively', 79),\n",
       " ('particle', 79),\n",
       " ('remove', 79),\n",
       " ('deal', 79),\n",
       " ('7', 79),\n",
       " ('national', 79),\n",
       " ('sit', 79),\n",
       " ('sport', 79),\n",
       " ('clearly', 78),\n",
       " ('layer', 78),\n",
       " ('wanted', 78),\n",
       " ('including', 78),\n",
       " ('flat', 78),\n",
       " ('roll', 78),\n",
       " ('regarding', 78),\n",
       " ('proper', 78),\n",
       " ('attention', 78),\n",
       " ('smell', 78),\n",
       " ('mouth', 78),\n",
       " ('outdoor', 78),\n",
       " ('eats', 78),\n",
       " ('damage', 77),\n",
       " ('feature', 77),\n",
       " ('normally', 77),\n",
       " ('apart', 77),\n",
       " ('density', 77),\n",
       " ('step', 77),\n",
       " ('nose', 77),\n",
       " ('brand', 77),\n",
       " ('window', 77),\n",
       " ('25', 77),\n",
       " ('worried', 77),\n",
       " ('rating', 77),\n",
       " ('arent', 76),\n",
       " ('extra', 76),\n",
       " ('billion', 76),\n",
       " ('reduce', 76),\n",
       " ('heading', 76),\n",
       " ('seat', 76),\n",
       " ('bunny', 76),\n",
       " ('directly', 75),\n",
       " ('resource', 75),\n",
       " ('13', 75),\n",
       " ('mentioned', 75),\n",
       " ('wasnt', 75),\n",
       " ('expensive', 75),\n",
       " ('bang', 75),\n",
       " ('require', 75),\n",
       " ('season', 75),\n",
       " ('worry', 75),\n",
       " ('easier', 75),\n",
       " ('alternative', 75),\n",
       " ('include', 75),\n",
       " ('ski', 75),\n",
       " ('cache', 75),\n",
       " ('website', 74),\n",
       " ('ring', 74),\n",
       " ('face', 74),\n",
       " ('environment', 74),\n",
       " ('family', 74),\n",
       " ('crash', 74),\n",
       " ('pig', 74),\n",
       " ('kept', 73),\n",
       " ('middle', 73),\n",
       " ('meter', 73),\n",
       " ('mention', 73),\n",
       " ('unfortunately', 73),\n",
       " ('wait', 73),\n",
       " ('clearance', 73),\n",
       " ('yard', 73),\n",
       " ('sock', 73),\n",
       " ('faster', 72),\n",
       " ('tool', 72),\n",
       " ('moment', 72),\n",
       " ('march', 72),\n",
       " ('final', 72),\n",
       " ('afraid', 72),\n",
       " ('coordinate', 71),\n",
       " ('giant', 71),\n",
       " ('million', 71),\n",
       " ('evidence', 71),\n",
       " ('obvious', 71),\n",
       " ('element', 71),\n",
       " ('prefer', 71),\n",
       " ('choice', 71),\n",
       " ('young', 71),\n",
       " ('23', 71),\n",
       " ('carrier', 71),\n",
       " ('airbus', 71),\n",
       " ('hit', 70),\n",
       " ('instruction', 70),\n",
       " ('observation', 70),\n",
       " ('strong', 70),\n",
       " ('guide', 70),\n",
       " ('extremely', 70),\n",
       " ('consideration', 70),\n",
       " ('11', 70),\n",
       " ('search', 70),\n",
       " ('guy', 70),\n",
       " ('suggestion', 70),\n",
       " ('worse', 70),\n",
       " ('controller', 70),\n",
       " ('watching', 69),\n",
       " ('understanding', 69),\n",
       " ('suitable', 69),\n",
       " ('piece', 69),\n",
       " ('magnetic', 69),\n",
       " ('c', 69),\n",
       " ('radius', 69),\n",
       " ('property', 69),\n",
       " ('particularly', 69),\n",
       " ('child', 69),\n",
       " ('river', 69),\n",
       " ('climber', 69),\n",
       " ('yes', 68),\n",
       " ('oxygen', 68),\n",
       " ('useful', 68),\n",
       " ('definition', 68),\n",
       " ('forest', 68),\n",
       " ('turned', 68),\n",
       " ('wall', 68),\n",
       " ('begin', 68),\n",
       " ('fixed', 68),\n",
       " ('pitch', 68),\n",
       " ('tip', 68),\n",
       " ('lake', 68),\n",
       " ('exercise', 68),\n",
       " ('wilderness', 68),\n",
       " ('venus', 67),\n",
       " ('properly', 67),\n",
       " ('physical', 67),\n",
       " ('fairly', 67),\n",
       " ('dwarf', 67),\n",
       " ('study', 67),\n",
       " ('supposed', 67),\n",
       " ('cover', 67),\n",
       " ('apparently', 67),\n",
       " ('log', 67),\n",
       " ('24', 67),\n",
       " ('giving', 67),\n",
       " ('medical', 67),\n",
       " ('alcohol', 67),\n",
       " ('vfr', 67),\n",
       " ('adopted', 67),\n",
       " ('future', 66),\n",
       " ('offer', 66),\n",
       " ('claim', 66),\n",
       " ('modern', 66),\n",
       " ('active', 66),\n",
       " ('moved', 66),\n",
       " ('limited', 66),\n",
       " ('occasionally', 66),\n",
       " ('14', 66),\n",
       " ('missing', 66),\n",
       " ('stall', 66),\n",
       " ('ifr', 66),\n",
       " ('weekend', 66),\n",
       " ('aggressive', 66),\n",
       " ('belay', 66),\n",
       " ('guinea', 66),\n",
       " ('stellar', 65),\n",
       " ('create', 65),\n",
       " ('date', 65),\n",
       " ('seeing', 65),\n",
       " ('setting', 65),\n",
       " ('sea', 65),\n",
       " ('remember', 65),\n",
       " ('happy', 65),\n",
       " ('item', 65),\n",
       " ('personal', 65),\n",
       " ('fur', 65),\n",
       " ('rotation', 64),\n",
       " ('typical', 64),\n",
       " ('scenario', 64),\n",
       " ('ball', 64),\n",
       " ('typically', 64),\n",
       " ('basically', 64),\n",
       " ('handle', 64),\n",
       " ('protection', 64),\n",
       " ('chicken', 64),\n",
       " ('military', 64),\n",
       " ('autopilot', 64),\n",
       " ('carrying', 64),\n",
       " ('leaf', 64),\n",
       " ('danger', 64),\n",
       " ('stove', 64),\n",
       " ('causing', 63),\n",
       " ('separate', 63),\n",
       " ('attempt', 63),\n",
       " ('arm', 63),\n",
       " ('lens', 63),\n",
       " ('wikipedia', 63),\n",
       " ('negative', 63),\n",
       " ('base', 63),\n",
       " ('drop', 63),\n",
       " ('chart', 63),\n",
       " ('appropriate', 63),\n",
       " ('toe', 63),\n",
       " ('mother', 63),\n",
       " ('bark', 63),\n",
       " ('trouble', 62),\n",
       " ('notice', 62),\n",
       " ('specie', 62),\n",
       " ('meet', 62),\n",
       " ('performance', 62),\n",
       " ('helicopter', 62),\n",
       " ('habit', 62),\n",
       " ('expected', 61),\n",
       " ('color', 61),\n",
       " ('deep', 61),\n",
       " ('improve', 61),\n",
       " ('changed', 61),\n",
       " ('additional', 61),\n",
       " ('straight', 61),\n",
       " ('continue', 61),\n",
       " ('student', 61),\n",
       " ('decided', 61),\n",
       " ('battery', 61),\n",
       " ('treatment', 61),\n",
       " ('orbital', 60),\n",
       " ('computer', 60),\n",
       " ('knowledge', 60),\n",
       " ('edge', 60),\n",
       " ('scale', 60),\n",
       " ('opinion', 60),\n",
       " ('link', 60),\n",
       " ('added', 60),\n",
       " ('building', 60),\n",
       " ('background', 60),\n",
       " ('late', 60),\n",
       " ('google', 60),\n",
       " ('total', 60),\n",
       " ('lack', 60),\n",
       " ('entire', 60),\n",
       " ('recommendation', 60),\n",
       " ('behaviour', 60),\n",
       " ('discovered', 59),\n",
       " ('observe', 59),\n",
       " ('possibility', 59),\n",
       " ('exact', 59),\n",
       " ('cluster', 59),\n",
       " ('record', 59),\n",
       " ('school', 59),\n",
       " ('story', 59),\n",
       " ('mode', 59),\n",
       " ('load', 59),\n",
       " ('stick', 59),\n",
       " ('catch', 59),\n",
       " ('propeller', 59),\n",
       " ('hunting', 59),\n",
       " ('anchor', 59),\n",
       " ('height', 58),\n",
       " ('potential', 58),\n",
       " ('roughly', 58),\n",
       " ('west', 58),\n",
       " ('ocean', 58),\n",
       " ('liquid', 58),\n",
       " ('money', 58),\n",
       " ('reaction', 58),\n",
       " ('showing', 58),\n",
       " ('access', 58),\n",
       " ('meat', 58),\n",
       " ('planetary', 57),\n",
       " ('infinite', 57),\n",
       " ('formation', 57),\n",
       " ('neutron', 57),\n",
       " ('formula', 57),\n",
       " ('unless', 57),\n",
       " ('quick', 57),\n",
       " ('phone', 57),\n",
       " ('thrust', 57),\n",
       " ('boat', 57),\n",
       " ('rescue', 57),\n",
       " ('biting', 57),\n",
       " ('kayak', 57),\n",
       " ('escape', 56),\n",
       " ('astronomer', 56),\n",
       " ('page', 56),\n",
       " ('city', 56),\n",
       " ('somewhat', 56),\n",
       " ('compared', 56),\n",
       " ('practical', 56),\n",
       " ('hope', 56),\n",
       " ('suggested', 56),\n",
       " ('tend', 56),\n",
       " ('cut', 56),\n",
       " ('decide', 56),\n",
       " ('sitting', 56),\n",
       " ('trained', 56),\n",
       " ('disease', 56),\n",
       " ('bone', 56),\n",
       " ('flea', 56),\n",
       " ('function', 55),\n",
       " ('bigger', 55),\n",
       " ('caused', 55),\n",
       " ('distant', 55),\n",
       " ('finger', 55),\n",
       " ('requires', 55),\n",
       " ('okay', 55),\n",
       " ('beginning', 55),\n",
       " ('indicate', 55),\n",
       " ('protect', 55),\n",
       " ('comment', 55),\n",
       " ('volume', 55),\n",
       " ('maintain', 55),\n",
       " ('suggest', 55),\n",
       " ('22', 55),\n",
       " ('bug', 55),\n",
       " ('identify', 55),\n",
       " ('16', 55),\n",
       " ('failure', 55),\n",
       " ('setup', 55),\n",
       " ('toilet', 55),\n",
       " ('grass', 55),\n",
       " ('table', 54),\n",
       " ('core', 54),\n",
       " ('closer', 54),\n",
       " ('motion', 54),\n",
       " ('game', 54),\n",
       " ('produce', 54)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
    "\n",
    "vect = CountVectorizer(tokenizer = LemmaTokenizer(), \n",
    "                       stop_words = 'english',  \n",
    "                       lowercase = True).fit(X)\n",
    "tdm = vect.transform(X)\n",
    "sum_words = tdm.sum(axis = 0)\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vect.vocabulary_.items()]\n",
    "words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
    "common_words = words_freq[:1000]\n",
    "common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5826x22201 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 201970 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Dimension Reduction\n",
    "Perform singular value decomposition on the term document matrix and retain 95% variability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components = tdm.shape[1] - 1, random_state = 1234)\n",
    "tsvd = svd.fit_transform(tdm)\n",
    "svd_var_ratios = svd.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2198"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a function\n",
    "def select_n_components(var_ratio, goal_var: float) -> int:\n",
    "    # Set initial variance explained so far\n",
    "    total_variance = 0.0\n",
    "    \n",
    "    # Set initial number of features\n",
    "    n_components = 0\n",
    "    \n",
    "    # For the explained variance of each feature:\n",
    "    for explained_variance in var_ratio:\n",
    "        \n",
    "        # Add the explained variance to the total\n",
    "        total_variance += explained_variance\n",
    "        \n",
    "        # Add one to the number of components\n",
    "        n_components += 1\n",
    "        \n",
    "        # If we reach our goal level of explained variance\n",
    "        if total_variance >= goal_var:\n",
    "            # End the loop\n",
    "            break\n",
    "            \n",
    "    # Return the number of components\n",
    "    return n_components\n",
    "\n",
    "select_n_components(svd_var_ratios, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_final = TruncatedSVD(n_components = select_n_components(svd_var_ratios, 0.95), random_state = 1234)\n",
    "tdm_final = svd_final.fit_transform(tdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5826, 2198)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Classifier\n",
    "Set aside 500 random discussions from entire dataset to serve as a hold-out test set.  Then fit a classifier to our training data using 10-fold cross validation to tune the parameters of the classifier of our choice (in this case, a random forest). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tdm_final, y, test_size = 500/tdm_final.shape[0], random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "\n",
    "# Define model & performance measure\n",
    "mdl = RandomForestClassifier(n_estimators = 20)\n",
    "accuracy = make_scorer(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8490424333458505"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random search for parameters\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 100),\n",
    "              \"min_samples_split\": sp_randint(2, 11),\n",
    "              \"min_samples_leaf\": sp_randint(1, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "rand_search = RandomizedSearchCV(mdl, param_distributions = param_dist, n_iter = 20, random_state = 1234, scoring = accuracy, \n",
    "                                 refit = True, cv = 10)\n",
    "mdl_1 = rand_search.fit(X_train, y_train)\n",
    "mdl_1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "att1_cv_accuracy = mdl_1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=85, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=10,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=20, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl_1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 85,\n",
       " 'min_samples_leaf': 10,\n",
       " 'min_samples_split': 2}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl_1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81999999999999995"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on hold-out test sample\n",
    "att1_y_pred = mdl_1.predict(X_test)\n",
    "att1_test_accuracy = accuracy_score(y_test, att1_y_pred)\n",
    "att1_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "  astronomy       0.89      0.89      0.89        96\n",
      "   aviation       0.81      0.87      0.84       139\n",
      "       beer       1.00      0.45      0.62        22\n",
      "   outdoors       0.71      0.80      0.75       131\n",
      "       pets       0.91      0.79      0.85       112\n",
      "\n",
      "avg / total       0.83      0.82      0.82       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "target_names = ['astronomy', 'aviation', 'beer', 'outdoors', 'pets']\n",
    "att1_class_report = classification_report(y_test, att1_y_pred, target_names = target_names)\n",
    "print(att1_class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 85   6   0   3   2]\n",
      " [  7 121   0  10   1]\n",
      " [  0   1  10  11   0]\n",
      " [  3  17   0 105   6]\n",
      " [  0   4   0  19  89]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "att1_conf_matrix = confusion_matrix(y_test, att1_y_pred, labels = target_names)\n",
    "print(att1_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store evaluation metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "att1_train_accuracy = accuracy_score(y_train, mdl_1.predict(X_train))\n",
    "att1_L = list(precision_recall_fscore_support(y_test, att1_y_pred, average = 'weighted'))\n",
    "metrics = pd.DataFrame([[1, att1_train_accuracy, att1_cv_accuracy, att1_test_accuracy, *att1_L[:3]]], \n",
    "                            columns = ['attempt', 'train accuracy', 'mean cv accuracy', 'test accuracy', 'test precision', \n",
    "                                       'test recall', 'test f1-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Attempt 2: Fit Classifier on Term Frequency Inverse Document Frequency Matrix\n",
    "Perform training on tf-idf matrix in place of the term document matrix citing only term frequency in an attempt to improve the classifier's performance.  A tf-idf matrix provides more information than the term frequency document matrix as it also takes into account the number of documents a word appears in.\n",
    "### Data Preprocessing\n",
    "Convert term document matrix using only word frequency in each document to term frequency inverse document frequency matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5826x22201 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 201970 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term Frequency Inverse Document Frequency \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer().fit(tdm)\n",
    "tfidf = tfidf_transformer.transform(tdm)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3971"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_svd = TruncatedSVD(n_components = tfidf.shape[1] - 1, random_state = 1234)\n",
    "tfidf_tsvd = tfidf_svd.fit_transform(tfidf)\n",
    "tfidf_svd_var_ratios = tfidf_svd.explained_variance_ratio_\n",
    "\n",
    "select_n_components(tfidf_svd_var_ratios, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_svd_final = TruncatedSVD(n_components = select_n_components(tfidf_svd_var_ratios, 0.95), random_state = 1234)\n",
    "tfidf_final = tfidf_svd_final.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Classifier on TF-IDF components\n",
    "Remove 500 documents to serve as hold out test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_final, y, test_size = 500/tfidf_final.shape[0], random_state = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have our model and evaluation metric that we outlined in attempt 1 (**mdl** and **accuracy**).  Additionally, we will use the same parameters distributions and random search cross validation as attempt 1, but we will fit it to the term frequency inverse document frequency matrix.  For reference on how we obtained **mdl**, **accuracy**, **param_dist** and **rand_search** see below:\n",
    "```\n",
    "# Define model & performance measure\n",
    "mdl = RandomForestClassifier(n_estimators = 20)\n",
    "accuracy = make_scorer(accuracy_score)\n",
    "\n",
    "# Random search for parameters\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 100),\n",
    "              \"min_samples_split\": sp_randint(2, 11),\n",
    "              \"min_samples_leaf\": sp_randint(1, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "rand_search = RandomizedSearchCV(mdl, param_distributions = param_dist, n_iter = 20, random_state = 1234, \n",
    "                                 scoring = accuracy, refit = True, cv = 10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91156590311678554"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit classifier using same algorithm, parameter distributions & random search parameters on tf-idf matrix\n",
    "mdl_2 = rand_search.fit(X_train, y_train)\n",
    "att2_cv_accuracy = mdl_2.best_score_\n",
    "att2_cv_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight=None,\n",
       "            criterion='entropy', max_depth=None, max_features=90,\n",
       "            max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "            min_samples_leaf=5, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl_2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'criterion': 'entropy',\n",
       " 'max_depth': None,\n",
       " 'max_features': 90,\n",
       " 'min_samples_leaf': 5,\n",
       " 'min_samples_split': 2}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl_2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89000000000000001"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on hold-out test sample\n",
    "att2_y_pred = mdl_2.predict(X_test)\n",
    "att2_test_accuracy = accuracy_score(y_test, att2_y_pred)\n",
    "att2_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "  astronomy       0.94      0.93      0.93        96\n",
      "   aviation       0.86      0.95      0.90       139\n",
      "       beer       1.00      0.64      0.78        22\n",
      "   outdoors       0.87      0.85      0.86       131\n",
      "       pets       0.89      0.88      0.89       112\n",
      "\n",
      "avg / total       0.89      0.89      0.89       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "target_names = ['astronomy', 'aviation', 'beer', 'outdoors', 'pets']\n",
    "att2_class_report = classification_report(y_test, att2_y_pred, target_names = target_names)\n",
    "print(att2_class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 89   4   0   2   1]\n",
      " [  3 132   0   2   2]\n",
      " [  2   3  14   3   0]\n",
      " [  1  10   0 111   9]\n",
      " [  0   4   0   9  99]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "att2_conf_matrix = confusion_matrix(y_test, att2_y_pred, labels = target_names)\n",
    "print(att2_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store evaluation metrics\n",
    "att2_train_accuracy = accuracy_score(y_train, mdl_2.predict(X_train))\n",
    "att2_L = list(precision_recall_fscore_support(y_test, att2_y_pred, average = 'weighted'))\n",
    "metrics = metrics.append(pd.DataFrame([[2, att2_train_accuracy, att2_cv_accuracy, att2_test_accuracy, *att2_L[:3]]], \n",
    "                                      columns = ['attempt', 'train accuracy', 'mean cv accuracy', 'test accuracy', \n",
    "                                                 'test precision', 'test recall', 'test f1-score']), \n",
    "                         ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attempt</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>mean cv accuracy</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>test precision</th>\n",
       "      <th>test recall</th>\n",
       "      <th>test f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.980473</td>\n",
       "      <td>0.849042</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.830855</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.819058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>0.911566</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.892493</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.888826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   attempt  train accuracy  mean cv accuracy  test accuracy  test precision  \\\n",
       "0        1        0.980473          0.849042           0.82        0.830855   \n",
       "1        2        0.999812          0.911566           0.89        0.892493   \n",
       "\n",
       "   test recall  test f1-score  \n",
       "0         0.82       0.819058  \n",
       "1         0.89       0.888826  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The final 2 attempts will mimic attempts 1 and 2, but with the train-test split occuring *before* dimension reduction takes place.  This is to ensure that no information from the hold-out test set is reflected in the training data as this could bias our result, thus providing an inaccurate evaluation of our classifier.    \n",
    "## Attempt 3: Train-Test Split prior to Dimension Reduction, Build Classifier on Term Frequency Matrix\n",
    "### Data Preprocessing\n",
    "We have already built our term document matrix measuring word frequency in each document.  Now we'll perform the train-test split and *then* apply dimension reduction before ultimately training our classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tdm, y, test_size = 500/tdm.shape[0], random_state = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2045"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components = X_train.shape[1] - 1, random_state = 1234)\n",
    "tsvd = svd.fit_transform(X_train)\n",
    "svd_var_ratios = svd.explained_variance_ratio_\n",
    "\n",
    "select_n_components(svd_var_ratios, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train = (5326, 2045)\n",
      "X_test = (500, 2045)\n"
     ]
    }
   ],
   "source": [
    "svd_final = TruncatedSVD(n_components = select_n_components(svd_var_ratios, 0.95), random_state = 1234)\n",
    "X_train_final = svd_final.fit_transform(X_train)\n",
    "X_test_final = svd_final.transform(X_test)\n",
    "\n",
    "print('X_train = ' + str(X_train_final.shape))\n",
    "print('X_test = ' + str(X_test_final.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Classifier\n",
    "Again, we will use the same algorithm, parameter distributions and random search parameters to fit our classifier.  However, we will be fitting our classifier on the data that was split prior to dimension reduction. Again, for reference, see below for how these items were derived:\n",
    "```\n",
    "# Define model & performance measure\n",
    "mdl = RandomForestClassifier(n_estimators = 20)\n",
    "accuracy = make_scorer(accuracy_score)\n",
    "\n",
    "# Random search for parameters\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 100),\n",
    "              \"min_samples_split\": sp_randint(2, 11),\n",
    "              \"min_samples_leaf\": sp_randint(1, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "rand_search = RandomizedSearchCV(mdl, param_distributions = param_dist, n_iter = 20, random_state = 1234, \n",
    "                                 scoring = accuracy, refit = True, cv = 10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85843034171986476"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit classifier using same algorithm, parameter distributions & random search parameter on data split prior to dimension reduction\n",
    "mdl_3 = rand_search.fit(X_train_final, y_train)\n",
    "att3_cv_accuracy = mdl_3.best_score_\n",
    "att3_cv_accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=85, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=10,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=20, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl_3.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 85,\n",
       " 'min_samples_leaf': 10,\n",
       " 'min_samples_split': 2}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl_3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84199999999999997"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on hold-out test sample\n",
    "att3_y_pred = mdl_3.predict(X_test_final)\n",
    "att3_test_accuracy = accuracy_score(y_test, att3_y_pred)\n",
    "att3_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "  astronomy       0.92      0.88      0.90        96\n",
      "   aviation       0.87      0.88      0.88       139\n",
      "       beer       1.00      0.73      0.84        22\n",
      "   outdoors       0.72      0.84      0.78       131\n",
      "       pets       0.89      0.79      0.83       112\n",
      "\n",
      "avg / total       0.85      0.84      0.84       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "target_names = ['astronomy', 'aviation', 'beer', 'outdoors', 'pets']\n",
    "att3_class_report = classification_report(y_test, att3_y_pred, target_names = target_names)\n",
    "print(att3_class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 84   2   0   8   2]\n",
      " [  4 123   0  11   1]\n",
      " [  0   2  16   4   0]\n",
      " [  2  11   0 110   8]\n",
      " [  1   4   0  19  88]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "att3_conf_matrix = confusion_matrix(y_test, att3_y_pred, labels = target_names)\n",
    "print(att3_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store evaluation metrics\n",
    "att3_train_accuracy = accuracy_score(y_train, mdl_3.predict(X_train_final))\n",
    "att3_L = list(precision_recall_fscore_support(y_test, att3_y_pred, average = 'weighted'))\n",
    "metrics = metrics.append(pd.DataFrame([[3, att3_train_accuracy, att3_cv_accuracy, att3_test_accuracy, *att3_L[:3]]], \n",
    "                                      columns = ['attempt', 'train accuracy', 'mean cv accuracy', 'test accuracy', \n",
    "                                                 'test precision', 'test recall', 'test f1-score']), \n",
    "                         ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attempt</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>mean cv accuracy</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>test precision</th>\n",
       "      <th>test recall</th>\n",
       "      <th>test f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.980473</td>\n",
       "      <td>0.849042</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.830855</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.819058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>0.911566</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.892493</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.888826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.986481</td>\n",
       "      <td>0.858430</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.850750</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.843437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   attempt  train accuracy  mean cv accuracy  test accuracy  test precision  \\\n",
       "0        1        0.980473          0.849042          0.820        0.830855   \n",
       "1        2        0.999812          0.911566          0.890        0.892493   \n",
       "2        3        0.986481          0.858430          0.842        0.850750   \n",
       "\n",
       "   test recall  test f1-score  \n",
       "0        0.820       0.819058  \n",
       "1        0.890       0.888826  \n",
       "2        0.842       0.843437  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Attempt 4: Train-Test Split prior to Dimension Reduction, Build Classifier on TF-IDF Matrix\n",
    "### Data Preprocessing\n",
    "Split tf-idf matrix into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf, y, test_size = 500/tfidf.shape[0], random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3703"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_svd = TruncatedSVD(n_components = X_train.shape[1] - 1, random_state = 1234)\n",
    "tfidf_tsvd = tfidf_svd.fit_transform(X_train)\n",
    "tfidf_svd_var_ratios = tfidf_svd.explained_variance_ratio_\n",
    "\n",
    "select_n_components(tfidf_svd_var_ratios, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train = (5326, 3703)\n",
      "X_test = (500, 3703)\n"
     ]
    }
   ],
   "source": [
    "tfidf_svd_final = TruncatedSVD(n_components = select_n_components(tfidf_svd_var_ratios, 0.95), random_state = 1234)\n",
    "X_train_final = tfidf_svd_final.fit_transform(X_train)\n",
    "X_test_final = tfidf_svd_final.transform(X_test)\n",
    "\n",
    "print('X_train = ' + str(X_train_final.shape))\n",
    "print('X_test = ' + str(X_test_final.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Classifier\n",
    "Reference:\n",
    "```\n",
    "# Define model & performance measure\n",
    "mdl = RandomForestClassifier(n_estimators = 20)\n",
    "accuracy = make_scorer(accuracy_score)\n",
    "\n",
    "# Random search for parameters\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 100),\n",
    "              \"min_samples_split\": sp_randint(2, 11),\n",
    "              \"min_samples_leaf\": sp_randint(1, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "rand_search = RandomizedSearchCV(mdl, param_distributions = param_dist, n_iter = 20, random_state = 1234, \n",
    "                                 scoring = accuracy, refit = True, cv = 10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91306796845662785"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit classifier using same algorithm, parameter distributions, random search parameters on tf-idf matrix split prior to dimension reduction\n",
    "mdl_4 = rand_search.fit(X_train_final, y_train)\n",
    "att4_cv_accuracy = mdl_4.best_score_\n",
    "att4_cv_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight=None,\n",
       "            criterion='entropy', max_depth=None, max_features=79,\n",
       "            max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "            min_samples_leaf=8, min_samples_split=4,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl_4.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'criterion': 'entropy',\n",
       " 'max_depth': None,\n",
       " 'max_features': 79,\n",
       " 'min_samples_leaf': 8,\n",
       " 'min_samples_split': 4}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl_4.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89000000000000001"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on hold-out test sample\n",
    "att4_y_pred = mdl_4.predict(X_test_final)\n",
    "att4_test_accuracy = accuracy_score(y_test, att4_y_pred)\n",
    "att4_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "  astronomy       0.91      0.90      0.91        96\n",
      "   aviation       0.90      0.93      0.91       139\n",
      "       beer       1.00      0.77      0.87        22\n",
      "   outdoors       0.90      0.82      0.86       131\n",
      "       pets       0.83      0.94      0.88       112\n",
      "\n",
      "avg / total       0.89      0.89      0.89       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "target_names = ['astronomy', 'aviation', 'beer', 'outdoors', 'pets']\n",
    "att4_class_report = classification_report(y_test, att4_y_pred, target_names = target_names)\n",
    "print(att4_class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 86   6   0   2   2]\n",
      " [  3 129   0   4   3]\n",
      " [  2   1  17   1   1]\n",
      " [  3   5   0 108  15]\n",
      " [  0   2   0   5 105]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "att4_conf_matrix = confusion_matrix(y_test, att4_y_pred, labels = target_names)\n",
    "print(att4_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store evaluation metrics\n",
    "att4_train_accuracy = accuracy_score(y_train, mdl_4.predict(X_train_final))\n",
    "att4_L = list(precision_recall_fscore_support(y_test, att4_y_pred, average = 'weighted'))\n",
    "metrics = metrics.append(pd.DataFrame([[4, att4_train_accuracy, att4_cv_accuracy, att4_test_accuracy, *att4_L[:3]]], \n",
    "                                      columns = ['attempt', 'train accuracy', 'mean cv accuracy', 'test accuracy', \n",
    "                                                 'test precision', 'test recall', 'test f1-score']), \n",
    "                         ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attempt</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>mean cv accuracy</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>test precision</th>\n",
       "      <th>test recall</th>\n",
       "      <th>test f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.980473</td>\n",
       "      <td>0.849042</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.830855</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.819058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>0.911566</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.892493</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.888826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.986481</td>\n",
       "      <td>0.858430</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.850750</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.843437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.997559</td>\n",
       "      <td>0.913068</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.892909</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.889623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   attempt  train accuracy  mean cv accuracy  test accuracy  test precision  \\\n",
       "0        1        0.980473          0.849042          0.820        0.830855   \n",
       "1        2        0.999812          0.911566          0.890        0.892493   \n",
       "2        3        0.986481          0.858430          0.842        0.850750   \n",
       "3        4        0.997559          0.913068          0.890        0.892909   \n",
       "\n",
       "   test recall  test f1-score  \n",
       "0        0.820       0.819058  \n",
       "1        0.890       0.888826  \n",
       "2        0.842       0.843437  \n",
       "3        0.890       0.889623  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt 4 seems to yield the best results across most or all of the metrics used for evaluation.  This will of course change from iteration to iteration due to the random search of optimal parameters for each attempt.  Regardless, in this iteration attempt 4 yields high accuracy in our training, cross-validation and test sets.  Additionally, the test recall across the 5 categories is the strongest in attempt 4, especially for **beer** posts which seem to be the most problematic when it comes to accurate classification as it has the smallest amount of documents associated with it.  It appears that using the term frequency inverse document frequency matrix for training addresses this issue by penalizing words that appear in many or most of the total documents.  Additionally, performing the train-test split prior to applying dimension reduction also improved our classifier's performance as the accuracy measures of our train, cross validation and test sets increased for both our term document matrix with only word frequency as well as our term frequency inverse document frequency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
